{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to represent\n",
    "- The actual utility functions\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jhr\\Anaconda3\\envs\\pycid\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "import pycid\n",
    "import numpy as np\n",
    "from pycid.core.mechanised_graph import MechanisedGraph\n",
    "from pycid.core.cpd import StochasticFunctionCPD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility_function_follower(defender: tuple, attacker: int) -> float:\n",
    "    \"\"\"From GT slides\"\"\"\n",
    "    attacker_reward = 2\n",
    "    follower_penalty = 1\n",
    "    u_follower = (1 - defender[attacker-1]) * attacker_reward + defender[attacker-1] * follower_penalty\n",
    "    return u_follower\n",
    "\n",
    "def utility_function_leader(d1: tuple, d2: int) -> float:\n",
    "    leader_reward = 2\n",
    "    leader_penalty = 1\n",
    "    u_leader = (1 - d1[d2-1]) * leader_penalty + d1[d2-1] * leader_reward\n",
    "    return u_leader\n",
    "\n",
    "def stochastic_cpd_to_dict(stochastic_cpd: StochasticFunctionCPD) -> dict:\n",
    "    \"\"\"Converts a stochastic cpd to a dictionary\"\"\"\n",
    "    return ast.literal_eval(str(stochastic_cpd).split(\"->\")[1])\n",
    "\n",
    "\n",
    "def calculate_utilities(stochastic_cpd: list[StochasticFunctionCPD]) -> tuple[float, float]:\n",
    "    \"\"\"Calculates the utility of a stochastic cpd\"\"\"\n",
    "    leader_strategy = stochastic_cpd_to_dict(stochastic_cpd[0])\n",
    "    follower_strategy = stochastic_cpd_to_dict(stochastic_cpd[1])\n",
    "\n",
    "    # find keys with nonzero values\n",
    "    leader_strategy = {key: value for key, value in leader_strategy.items() if value > 0}\n",
    "\n",
    "    # calculate weighted utility for each\n",
    "    leader_utility = 0\n",
    "    follower_utility = 0 \n",
    "    for leader_strat, leader_prob in leader_strategy.items():\n",
    "        for follower_strat, follower_prob in follower_strategy.items():\n",
    "            leader_utility += leader_prob * follower_prob * utility_function_leader(leader_strat, follower_strat)\n",
    "            follower_utility += leader_prob * follower_prob * utility_function_follower(leader_strat, follower_strat)\n",
    "    \n",
    "    return leader_utility, follower_utility\n",
    "\n",
    "def find_best_nash_equilibrium(nash_eq: list[StochasticFunctionCPD]) -> tuple[StochasticFunctionCPD, float]:\n",
    "    \"\"\"Finds the best nash equilibrium\"\"\"\n",
    "    best_nash_eq = None\n",
    "    best_utility = 0\n",
    "    for eq in nash_eq:\n",
    "        leader_utility, follower_utility = calculate_utilities(eq)\n",
    "        if leader_utility > best_utility:\n",
    "            best_utility = leader_utility\n",
    "            assert isinstance(best_utility, float)\n",
    "            best_nash_eq = eq\n",
    "    return best_nash_eq, best_utility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Simple MACID setup\n",
    "macid = pycid.MACID(\n",
    "    [(\"D1\", \"U1\"), (\"D1\", \"U2\"), (\"D2\", \"U1\"), (\"D2\", \"U2\")],\n",
    "    agent_decisions={1: [\"D1\"], 2: [\"D2\"]},\n",
    "    agent_utilities={1: [\"U1\"], 2: [\"U2\"]},\n",
    ")\n",
    "\n",
    "# Discretise domains to fit with PyCID (It is not possible to use continuous domains in pygambit, right?)\n",
    "PRECISION = 0.2\n",
    "CONSTRAINT = 2.6\n",
    "d1_domain = [(a, b, c) for a in np.arange(0, 1.1, PRECISION) for b in np.arange(0, 1.1, PRECISION) for c in np.arange(0, 1.1, PRECISION) if a + b + c == CONSTRAINT]\n",
    "d2_domain = list(range(1, 4))\n",
    "\n",
    "macid.add_cpds(\n",
    "    D1 = d1_domain,\n",
    "    D2 = d2_domain,\n",
    "    U1 = lambda D1, D2: utility_function_leader(D1, D2),\n",
    "    U2 = lambda D1, D2: utility_function_follower(D1, D2),\n",
    ")\n",
    "\n",
    "\n",
    "mech = MechanisedGraph(macid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhr\\pycid\\pycid\\core\\macid_base.py:59: UserWarning: adding DecisionDomain to non-decision node D1\n",
      "  warn(f\"adding DecisionDomain to non-decision node {variable}\")\n",
      "C:\\Users\\jhr\\pycid\\pycid\\core\\macid_base.py:59: UserWarning: adding DecisionDomain to non-decision node D2\n",
      "  warn(f\"adding DecisionDomain to non-decision node {variable}\")\n"
     ]
    }
   ],
   "source": [
    "# solve for all nash equilibria\n",
    "nash_eq = macid.get_ne(solver=\"enummixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycid.core.cpd import StochasticFunctionCPD\n",
    "\n",
    "def any_high_prob(CPD: StochasticFunctionCPD, threshold=0.9) -> bool:\n",
    "    return any(probability for probability in CPD.get_values() if probability > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.8666666666666671, 1.1333333333333333),\n",
       " (1.8666666666666667, 1.1333333333333333),\n",
       " (1.8666666666666667, 1.1333333333333333),\n",
       " (1.8666666666666667, 1.1333333333333333),\n",
       " (1.866666666666667, 1.1333333333333335)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[calculate_utilities(eq) for eq in nash_eq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8666666666666671 for the leader\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#sample_nash_eqs = random.sample(nash_eq, 200)\n",
    "\n",
    "best_equilibrium, best_utility = find_best_nash_equilibrium(nash_eq)\n",
    "print(f\"{best_utility} for the leader\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c9fc4f94fa3417a06aff16283d7f0aaf7807c9785340e38d545152ddc0502bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
